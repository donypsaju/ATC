name: Roster Data Scraper

# This controls when the action will run
on:
  # Run on a schedule (cron: min, hour, day(month), month, day(week))
  # '0 2 * * 0' means "at 02:00 on Sunday" (every 7 days)
  schedule:
    - cron: '0 2 * * 0'

  # This line allows you to run the workflow manually from the "Actions" tab
  workflow_dispatch:

# This defines the actual job
jobs:
  scrape-and-commit:
    # Use the latest version of Ubuntu as the virtual machine
    runs-on: ubuntu-latest

    steps:
      # Step 1: Check out your repository's code
      - name: Check out repository
        uses: actions/checkout@v4

      # Step 2: Set up the Python environment
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      # Step 3: Install the required Python libraries
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Run your Python scraper script
      # This will create/update the 'roster_data.json' file
      - name: Run scraper
        run: python scrape_roster.py

      # Step 5: Commit the new 'roster_data.json' file back to the repo
      - name: Commit data file
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Automated: Update roster_data.json"
          
          # This ensures we *only* commit the data file
          file_pattern: roster_data.json
          
          commit_user_name: GitHub Actions
          commit_user_email: actions@github.com
